{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KWG-cy_Spm1d",
        "4jK-fKQYKpDF",
        "NQdrrhbODO8A",
        "D7-EOrRDFgq4",
        "_Jzdq40zqa8n",
        "Tgwt7-DaJQFu",
        "pxtPugaq2dqA",
        "mU7GRkOBMxGz",
        "V1M3sCmG5tvQ"
      ],
      "authorship_tag": "ABX9TyOI/j7/eevpH+cgwXedMddd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhan16/colabs/blob/master/ml/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR: Model"
      ],
      "metadata": {
        "id": "KWG-cy_Spm1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model\n",
        "Given data of n observations {**x<sub>i</sub>**, y<sub>i</sub>}<sub>i=1:n</sub> with y<sub>i</sub> a scalar response and **x<sub>i</sub>** a column vector of size p:\n",
        "* y<sub>i</sub> = β<sub>1</sub>x<sub>i1</sub> + ... + β<sub>p</sub>x<sub>ip</sub> + ε<sub>i</sub> = x<sub>i</sub><sup>T</sup>β + ε<sub>i</sub>\n",
        "\n",
        "* or, in vector form: y<sub>i</sub> = **x<sub>i</sub>**<sup>T</sup>**β** + ε<sub>i</sub>\n",
        "\n",
        "* or, stacking the n equations together in matrix notation: **y** = X**β** + **ε**\n",
        "---"
      ],
      "metadata": {
        "id": "51lMvLcRPNMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ordinary Least Squares Solution\n",
        "Find the coefficients **β** which fit the equations \"best\", **$\\hat{β}$** = arg min L(**β**), where\n",
        "\n",
        "* Loss function L(**β**) = ||**y** - X**β**||<sup>2</sup>\n",
        "\n",
        "* The solution is: **$\\hat{β}$** = (X<sup>T</sup>X)<sup>-1</sup> X<sup>T</sup>**y**\n",
        "\n",
        "where (X<sup>T</sup>X)<sup>-1</sup> X<sup>T</sup> is  the Moore–Penrose pseudoinverse matrix of X\n",
        "\n",
        "##### References\n",
        "* https://en.wikipedia.org/wiki/Ordinary_least_squares#Linear_model\n",
        "---\n"
      ],
      "metadata": {
        "id": "R-S1KGH0AjvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gradient of the loss function\n",
        "\n",
        "∇<sub>**β**</sub>L = 2X<sup>T</sup>(X**β** - **y**)\n",
        "\n",
        "#### References\n",
        "* https://www.inf.ed.ac.uk/teaching/courses/mlpr/2016/notes/w3b_regression_gradients.pdf\n",
        "* https://d2l.ai/chapter_linear-regression/linear-regression.html#sec-linear-regression"
      ],
      "metadata": {
        "id": "k46Y1IJYAZtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "4jK-fKQYKpDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data, np and plot utils"
      ],
      "metadata": {
        "id": "K75cezqGYtdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/validation_and_test_sets.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=validation_tf2-colab&hl=en\n",
        "import numpy as np\n",
        "\n",
        "def make_simple_regression_data(beta, n):\n",
        "  np.random.seed(100) # seed random number generator\n",
        "  x = 1000 * np.random.rand(n)\n",
        "  e = 100 * np.random.rand(len(x))\n",
        "  y = beta[0] + x*beta[1] + e\n",
        "  print('make_simple_regression_data beta:', beta)\n",
        "  return (np.array([[e] for e in x]), y)\n",
        "\n",
        "from sklearn import datasets\n",
        "def make_regression_data(n_samples, n_features, intercept=0):\n",
        "  x, y, coef = datasets.make_regression(n_samples=n_samples,#number of samples\n",
        "                                      n_features=n_features,#number of features\n",
        "                                      n_informative=n_features,#number of useful features \n",
        "                                      bias=intercept,\n",
        "                                      noise=10,#bias and standard deviation of the guassian noise\n",
        "                                      coef=True,#true coefficient used to generated the data\n",
        "                                      random_state=0) #set for sa, plot utilsts for each run\n",
        "  if n_features == 1: # ugh, coef is a scalar for 1 feature, vector for > 1\n",
        "    coef = [coef]\n",
        "  beta = np.insert(coef, 0, intercept, axis=0)\n",
        "  print('make_regression_data beta:', beta)\n",
        "  return x, y, beta"
      ],
      "metadata": {
        "id": "Kq-utI6FnaRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale(x, a, b):\n",
        "  '''Scale numpy array x to be between a and b'''\n",
        "  range = np.amax(x) - np.amin(x)\n",
        "  return x / range * (b - a) + a\n",
        "\n",
        "''' Returns matrix with first column as 1's from x, where x is an array of arrays'''\n",
        "def add_intercept(x):\n",
        "  x_plus=np.vstack((np.ones(len(x)), np.array(x).T)).T\n",
        "  return x_plus\n",
        "\n",
        "def remove_intercept(x):\n",
        "  return x[:, 1:]\n",
        "\n",
        "# Return true if the first column is all 1s\n",
        "def has_intercept(x):\n",
        "  return np.all(x[:, 0] == 1)"
      ],
      "metadata": {
        "id": "ZOK8emSCNk25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict\n",
        "\n",
        "def lr_plot2D(x, y, beta):\n",
        "  intercept = has_intercept(x)\n",
        "  if intercept:\n",
        "    x = remove_intercept(x)\n",
        "  x = x.flatten()  # convert into 1D\n",
        "  plt.plot(x, y, 'o', label='data')\n",
        "\n",
        "  xx = np.linspace(min(x), max(y), 11)\n",
        "  xx = x[:, np.newaxis] # convert into 2D\n",
        "  yy = lr_predict_y(add_intercept(xx) if intercept else xx, beta)\n",
        "\n",
        "  if intercept:\n",
        "    label='fit, $y = {:.2f} + {:.2f} * x$'.format(beta[0], beta[1])\n",
        "  else:\n",
        "    label='fit, $y = {:.2f} * x$'.format(beta[0])\n",
        "  plt.plot(xx, yy, label=label)\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.legend(framealpha=1, shadow=True)\n",
        "  plt.grid(alpha=0.25)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def lr_plot3D(x, y, beta):\n",
        "  # https://www.kaggle.com/code/spidy20/3d-visualization-of-multiple-linear-regression/notebook\n",
        "  # https://gist.github.com/aricooperdavis/c658fc1c5d9bdc5b50ec94602328073b\n",
        "  intercept = has_intercept(x)\n",
        "  if intercept:\n",
        "    x = remove_intercept(x)\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.set_xlabel(\"X1\")\n",
        "  ax.set_ylabel(\"X2\")\n",
        "  ax.set_zlabel(\"y\")\n",
        "  ax.scatter(x[:,0], x[:,1], y, marker='.', color='red')\n",
        "\n",
        "  x1_min, x1_max = min(x[:,0]), max(x[:,0])\n",
        "  x2_min, x2_max = min(x[:,1]), max(x[:,1])\n",
        "  xs = scale(np.tile(np.arange(101), (101,1)), x1_min, x1_max)\n",
        "  ys = scale(np.tile(np.arange(101), (101,1)).T, x2_min, x2_max)\n",
        "  if intercept:\n",
        "    zs = beta[0] + xs*beta[1]+ ys*beta[2]\n",
        "  else:\n",
        "    zs = xs*beta[0]+ ys*beta[1]\n",
        "  ax.plot_surface(xs,ys,zs, alpha=0.3)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_losscurves(losscurves: Dict):\n",
        "  fmts = {\n",
        "      'test_loss': 'g',\n",
        "      'batch_loss': 'b',\n",
        "      'train_loss_by_batch': 'r',\n",
        "      'train_loss_by_epoch': 'r+',\n",
        "  }\n",
        "  for label, losscurve in losscurves.items():\n",
        "    print(label, losscurve)\n",
        "    plt.plot(losscurve, fmts[label], label=label)\n",
        "  plt.legend(framealpha=1, shadow=True)\n",
        "  plt.grid(alpha=0.25)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ElI14AV4lrWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LR Utils"
      ],
      "metadata": {
        "id": "q3SapWIzZYtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "''' x is an array of arrays, beta and y are arrays'''\n",
        "def lr_predict_y(x, beta):\n",
        "  return x.dot(beta)\n",
        "\n",
        "# loss for current values of beta. we use mse so that batch and train/test errors are comparable\n",
        "def lr_loss(X_mat, y, beta):\n",
        "  return lr_sse(X_mat, y, beta) / len(y)\n",
        "\n",
        "# sum of squared errors\n",
        "def lr_sse(X_mat, y, beta):\n",
        "  h = lr_predict_y(X_mat, beta)\n",
        "  J = np.dot((h - y).transpose(), (h - y))\n",
        "  return J\n",
        "\n",
        "''' y and y_pred are arrays'''\n",
        "def show_metrics(y, y_pred):\n",
        "  print('mse: {:.4f} r2: {:.4f}'.format(mean_squared_error(y, y_pred), r2_score(y, y_pred)))\n",
        "\n",
        "def lr_show_results(model_name, x, y, beta, metrics = True, plot = True):\n",
        "  print('\\n============ {} ============'.format(model_name))\n",
        "  print('beta: {}'.format(beta))\n",
        "  if metrics:\n",
        "    show_metrics(y, lr_predict_y(x, beta))\n",
        "\n",
        "  intercept = has_intercept(x)\n",
        "  num_features = len(beta) - 1 if intercept else len(beta)\n",
        "  if plot:\n",
        "    if num_features == 1:\n",
        "      lr_plot2D(x, y, beta)\n",
        "    elif num_features == 2:\n",
        "      lr_plot3D(x, y, beta)"
      ],
      "metadata": {
        "id": "t0jrT_wZaUUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR: Ordinary Least Squares method"
      ],
      "metadata": {
        "id": "uz1kazv7Yhhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### OLS Impl"
      ],
      "metadata": {
        "id": "NQdrrhbODO8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **$\\hat{β}$** = (X<sup>T</sup>X)<sup>-1</sup> X<sup>T</sup>**y**. https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/\n",
        "2. **$\\hat{β}$** = X<sup>pseudo-inverse</sup> **y**\n",
        "3. scipy.linalg.lstsq https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html#scipy.linalg.lstsq\n",
        "4. sklearn LinearRegression https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/"
      ],
      "metadata": {
        "id": "9GdvV3beostQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OLS library\n",
        "import numpy as np\n",
        "from scipy.linalg import lstsq\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/\n",
        "def ols_with_inverse(X_mat, y):\n",
        "  return np.linalg.inv(X_mat.T.dot(X_mat)).dot(X_mat.T).dot(y)\n",
        "\n",
        "# 2.\n",
        "def ols_with_pseudoinverse(X_mat, y):\n",
        "  return np.linalg.pinv(X_mat).dot(y)\n",
        "\n",
        "# 3. https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html#scipy.linalg.lstsq\n",
        "def ols_with_numpy_lstsq(X_mat, y):\n",
        "  beta_hat, res, rnk, s = lstsq(X_mat, y)\n",
        "  return beta_hat\n",
        "  \n",
        "# 4. sklearn.linear_model.LinearRegression https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/\n",
        "def ols_with_sklearn(X_mat, y, fit_intercept=False):\n",
        "  lr = LinearRegression(fit_intercept=fit_intercept).fit(x, y)\n",
        "  if fit_intercept:\n",
        "    beta_hat = np.insert(lr.coef_, 0, lr.intercept_, axis=0)\n",
        "  else:\n",
        "    beta_hat = lr.coef_\n",
        "  return beta_hat\n",
        "\n",
        "def run_and_show_olss(x, y, inv=False, pinv=False, lstsq=False, sklearn=True, fit_intercept=False):\n",
        "  if fit_intercept:\n",
        "    x=add_intercept(x)\n",
        "  if inv:\n",
        "    lr_show_results('ols_with_inverse', x, y, ols_with_inverse(x, y))\n",
        "  if pinv:\n",
        "    lr_show_results('ols_with_pseudoinverse', x, y, ols_with_pseudoinverse(x, y))\n",
        "  if lstsq:\n",
        "    lr_show_results('ols_with_numpy_lstsq', x, y, ols_with_numpy_lstsq(x, y))\n",
        "  if sklearn:\n",
        "    lr_show_results('ols_with_sklearn', x, y, ols_with_sklearn(x, y, fit_intercept=fit_intercept))"
      ],
      "metadata": {
        "id": "0UBsSpChprbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### OLS Runs"
      ],
      "metadata": {
        "id": "zb83cGsEE8Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x, y) = make_simple_regression_data([50, 3], 100)\n",
        "run_and_show_olss(x, y, inv=True, pinv=True, lstsq=True, sklearn=True, fit_intercept=True)"
      ],
      "metadata": {
        "id": "FomYIOMKVQJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_show_olss(x, y, inv=True, pinv=True, lstsq=True, sklearn=True, fit_intercept=False)"
      ],
      "metadata": {
        "id": "FmBHclYI8EbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, coef = make_regression_data(n_samples=100, n_features=1, intercept=100)\n",
        "run_and_show_olss(x, y, fit_intercept=True)"
      ],
      "metadata": {
        "id": "NPKctrrvXYrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_show_olss(x, y, fit_intercept=False)"
      ],
      "metadata": {
        "id": "AIoInSUyO6CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, coef = make_regression_data(n_samples=1000, n_features=2, intercept=300)\n",
        "run_and_show_olss(x, y, fit_intercept=True)"
      ],
      "metadata": {
        "id": "E4X31-r9AKb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_show_olss(x, y, fit_intercept=False)\n"
      ],
      "metadata": {
        "id": "HJ6n_XA2POOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR: Gradient descent\n",
        "\n"
      ],
      "metadata": {
        "id": "mNDFctL4Mw52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LR GradD: Common functions"
      ],
      "metadata": {
        "id": "D7-EOrRDFgq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradient Descent library\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils import gen_batches\n",
        "\n",
        "# function to perform mini-batch gradient descent\n",
        "'''x is an array of arrays'''\n",
        "def gradient_descent(x, y, grad_fn, loss_fn, \n",
        "                     epochs=3, batch_size=32, learning_rate=0.001,\n",
        "                     model='Gradient Descent'):\n",
        "  print('{} epochs:{} batch_size:{}, learning_rate:{}'.format(model, epochs, batch_size, learning_rate))\n",
        "  beta = np.zeros(x.shape[1])\n",
        "  batch_losses = []\n",
        "  train_losses_by_batch = []\n",
        "  train_losses_by_epoch = []\n",
        "  for itr in range(epochs):\n",
        "    x, y = shuffle(x, y)\n",
        "    for batch_slice in gen_batches(len(y), batch_size):\n",
        "      x_mini, y_mini = x[batch_slice], y[batch_slice]\n",
        "      beta = beta - learning_rate * grad_fn(x_mini, y_mini, beta)  \n",
        "      batch_losses.append(loss_fn(x_mini, y_mini, beta))\n",
        "      train_losses_by_batch.append(loss_fn(x, y, beta))\n",
        "    train_losses_by_epoch.append(loss_fn(x, y, beta))    \n",
        "\n",
        " \n",
        "  losscurves = {\n",
        "      'batch_loss': batch_losses,\n",
        "      'train_loss_by_batch' : train_losses_by_batch,\n",
        "      'train_loss_by_epoch': train_losses_by_epoch\n",
        "  }\n",
        "  return beta, losscurves"
      ],
      "metadata": {
        "id": "dKQNGHOCwPxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LR GradD: Analytical gradient"
      ],
      "metadata": {
        "id": "_Jzdq40zqa8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LR GradD: Analytical gradient Impl"
      ],
      "metadata": {
        "id": "Tgwt7-DaJQFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Gradient of the loss function: ∇<sub>**β**</sub>L = 2X<sup>T</sup>(X**β** - **y**)\n",
        "* https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/\n"
      ],
      "metadata": {
        "id": "XYiWE3zpPD8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "# function to compute gradient for regression loss function\n",
        "def lr_grad(X_mat, y, beta):\n",
        "  h = lr_predict_y(X_mat, beta)\n",
        "  grad = np.dot(X_mat.transpose(), (h - y))\n",
        "  return grad\n",
        "\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils import gen_batches\n",
        "# function to perform mini-batch gradient descent\n",
        "'''x is an array of arrays without the intercept term'''\n",
        "def lr_gradient_descent(x, y, epochs=3, batch_size=32, learning_rate=0.001):\n",
        "  return gradient_descent(x, y, lr_grad, lr_loss, \n",
        "                          epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                          model='LR GradD: Analytical Gradient')\n",
        "\n",
        "def run_and_show_lrgd(x, y, **kwargs):\n",
        "  x=add_intercept(x)\n",
        "  beta, losscurves = lr_gradient_descent(x, y, **kwargs)\n",
        "  lr_show_results('LR GradD: Analytical Gradient', x, y, beta)\n",
        "  plot_losscurves(losscurves)"
      ],
      "metadata": {
        "id": "mG9hTtRYFGpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LR GradD: Analytical gradient Runs"
      ],
      "metadata": {
        "id": "IFg79xy6SWuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, coef = make_regression_data(n_samples=100, n_features=1)\n",
        "run_and_show_lrgd(x, y, batch_size=32, epochs=20, learning_rate=.01)\n",
        "run_and_show_olss(x, y)"
      ],
      "metadata": {
        "id": "Cq1UXYnv4YzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, coef = make_regression_data(n_samples=1000, n_features=10, intercept=3)\n",
        "# run_and_show_lrgd(x, y)\n",
        "run_and_show_lrgd(x, y, batch_size=32, epochs=10, learning_rate=.001)\n",
        "run_and_show_olss(x, y)"
      ],
      "metadata": {
        "id": "hw-Yb2tgbetZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LR GradD: Forward-mode auto-diff\n"
      ],
      "metadata": {
        "id": "pxtPugaq2dqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Multivariate chain rule\n",
        "\n",
        "The crux of autodiff is the multivariate chain rule. Suppose we have\n",
        "* a function f in two variables\n",
        "* we want to compute $\\dfrac{\\mathrm{d}f(x(t),y(t))}{\\mathrm{d}t}$\n",
        "\n",
        "Changing t slightly has two effects: it changes x slightly,\n",
        "and it changes y slightly. Each of these effects causes a slight change to f. For infinitesimal changes, these effects combine additively. The Chain Rule is therefore:\n",
        "\n",
        "$\\dfrac{\\mathrm{d}f(x(t),y(t))}{\\mathrm{d}t} = \\dfrac{∂f}{∂x} \\dfrac{dx}{dt} + \\dfrac{∂f}{∂y} \\dfrac{dy}{dt}$\n",
        "\n",
        "##### References\n",
        "* https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L06%20Backpropagation.pdf\n",
        "* https://en.wikipedia.org/wiki/Chain_rule#Multivariable_case\n"
      ],
      "metadata": {
        "id": "FiMYjdZl2EJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Forward mode\n",
        "\n",
        "We are going to calculate the function evaluation and its derivative for an arbitrary function f given some data x using operator overloading i.e. we are going to change the default behavior of the core arithmetic operators such as +, -, /, * and the mathematical functions max, min, mean, log, cos, sin, and so-on, for a custom class.\n",
        "\n",
        "We create a class Variable which keeps track of function evaluation and the derivative using the chain rule.\n",
        "\n",
        "##### References\n",
        "*  https://mdrk.io/introduction-to-automatic-differentiation/\n",
        "* https://mdrk.io/introduction-to-automatic-differentiation-part2/"
      ],
      "metadata": {
        "id": "wmVgVHX-F5Ao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Implementation"
      ],
      "metadata": {
        "id": "x45sTgMZ89YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Forward mode auto-diff library\n",
        "\n",
        "class Variable():\n",
        "  def __init__(self, f, d = 0):\n",
        "    self.f = f\n",
        "    self.d = d\n",
        "    \n",
        "  def __repr__(self):\n",
        "    return f\"Variable(f={self.f}, d={self.d})\"\n",
        "    \n",
        "  def __add__(self, other): # other + self\n",
        "    return Variable(self.f + other.f, self.d + other.d)\n",
        "\n",
        "  def __sub__(self, other): # self - other\n",
        "    return Variable(self.f - other.f, self.d - other.d)\n",
        "\n",
        "  def __mul__(self, other): # self * other\n",
        "    '''uses product rule'''\n",
        "    return Variable(self.f * other.f, other.f * self.d + other.d * self.f)\n",
        "\n",
        "  def __truediv__(self, other): # self / other\n",
        "    '''uses differentiation rule for div'''\n",
        "    return Variable(self.f / other.f, (self.d * other.f - self.f * other.d) / other.f**2)\n",
        "\n",
        "  def __radd__(self, other): # other + self\n",
        "    return other + self\n",
        "  \n",
        "  def __rsub__(self, other): # other - self\n",
        "    return other - self\n",
        "\n",
        "  def __rmul__(self, other): # other * self\n",
        "    return other * self\n",
        "\n",
        "  def __rtruediv__(self, other): # other / self\n",
        "    return other/self"
      ],
      "metadata": {
        "id": "WG265jvG8v1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, a, b):\n",
        "  return x * a / (x * x + b)\n",
        "\n",
        "# x=24, a=22,b=2\n",
        "x = Variable(24, 1) # d = 1 since derivatve of x wrt x is 1\n",
        "a = Variable(22, 0) # d = 0 since derivatve of a wrt x is 0\n",
        "b = Variable(2, 0) # d = 0 since derivatve of b wrt x is 0\n",
        "\n",
        "print(f(x, a, b))"
      ],
      "metadata": {
        "id": "Ewt-jxDUGSEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LR GradD: JAX\n"
      ],
      "metadata": {
        "id": "mU7GRkOBMxGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "* https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/validation_and_test_sets.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=validation_tf2-colab&hl=en#scrollTo=FBhNIdUatOU6\n",
        "\n",
        "* https://colab.research.google.com/github/kaustubholpadkar/Predicting-House-Price-using-Multivariate-Linear-Regression/blob/master/Multivariate_Linear_Regression_Python.ipynb\n",
        "\n",
        "* https://www.coursera.org/projects/regression-automatic-differentiation-tensorflow\n",
        "\n"
      ],
      "metadata": {
        "id": "RD7wkqxjTWOi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiywV4hNLpHc"
      },
      "outputs": [],
      "source": [
        "#@title Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "metadata": {
        "id": "QIwZH0faTdic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR: Tensorflow\n"
      ],
      "metadata": {
        "id": "W_nXwuI79c8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML"
      ],
      "metadata": {
        "id": "V1M3sCmG5tvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Engineering\n",
        "\n",
        "* Load data\n",
        "* Shuffle\n",
        "* Split into training, validation and test. \n",
        "* Scale label column to meaningful values\n",
        "\n",
        "Features\n",
        "* Bucketize, cross \n",
        "* Scale features - min max or z scale\n",
        "\n",
        "\n",
        "References\n",
        "\n",
        "* https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/representation_with_a_feature_cross.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=representation_tf2-colab&hl=en\n"
      ],
      "metadata": {
        "id": "O-CT6ZDe5zLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define and Train Models\n",
        "* create model\n",
        "* train model\n",
        "* plot loss curves\n"
      ],
      "metadata": {
        "id": "Mup-lMKY59zw"
      }
    }
  ]
}